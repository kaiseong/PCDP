# train_diffusion_SPEC_mono_worksapce.yaml
defaults:
  - task: SPEC_stack

name: train_diffusion_SPEC_mono
_target_: pcdp.workspace.train_diffusion_SPEC_mono_workspace.TrainSPECWorkspace

task_name: ${task.name}
shape_meta: ${task.shape_meta}
exp_name: "debug"

# horizon: 4
horizon: 16
n_obs_steps: 1
# n_action_steps: 3
n_action_steps: 8
n_latency_steps: 0
dataset_obs_steps: ${n_obs_steps}
keypoint_visible_rate: 1.0
obs_as_global_cond: True

policy:
  _target_: pcdp.policy.diffusion_SPEC_policy_mono.SPECPolicyMono
  num_action: ${horizon}
  input_dim: 7
  obs_feature_dim: 512
  hidden_dim: 512
  nheads: 8
  num_encoder_layers: 4
  num_decoder_layers: 1
  dropout: 0.1
  #
  enable_c_gate: False


ema:
  _target_: pcdp.model.diffusion.ema_model.EMAModel
  update_after_step: 0
  inv_gamma: 1.0
  power: 0.75
  min_value: 0.0
  max_value: 0.9999

dataloader:
  batch_size: 180
  num_workers: 20
  shuffle: True
  # pin_memory: True
  # persistent_workers: True
  drop_last: True


val_dataloader:
  batch_size: 128
  num_workers: 10
  shuffle: False
  # pin_memory: True
  # persistent_workers: False

optimizer:
  _target_: torch.optim.AdamW
  lr: 3.0e-4
  betas: [0.95, 0.999]
  eps: 1.0e-8
  weight_decay: 1.0e-6

training:
  device: "cuda:0"
  seed: 233
  debug: False
  resume: True
  lr_warmup_steps: 1000
  num_epochs: 120
  use_ema: False
  rollout_every: 1
  save_epochs: 20
  tqdm_interval_sec: 1.0
  validation_every: 5
  # normalization
  translation:
    - [-0.142, 0.585]    # X range (meters)
    - [-0.500, 0.330]   # Y range (meters)  
    - [-0.085, 0.470]    # Z range (meters)
  
logging:
  group: ${exp_name}
  id: null
  mode: online
  name: "SPEC_mono${now:%Y.%m.%d}- (fused, Filter=on)"
  project: Test
  resume: true
  tags:
  - SPEC_mono_NSCL

checkpoint:
  save_ckpt: True
  save_last_ckpt: True # this only saves when save_ckpt is True
  save_last_snapshot: False

multi_run:
  run_dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
  wandb_name_base: ${now:%Y.%m.%D-%H.%M.%S}_${name}_${task_name}

hydra:
  job:
    override_dirname: ${name}
  run:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
  sweep:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
    subdir: ${hydra.job.num}
