defaults:
  - task: PCDP_stack

name: train_diffusion_PCDP
_target_: pcdp.workspace.train_diffusion_PCDP_workspace.TrainPCDPWorkspace

task_name: ${task.name}
shape_meta: ${task.shape_meta}
exp_name: "debug"

# horizon: 4
horizon: 16
n_obs_steps: 1
# n_action_steps: 3
n_action_steps: 8
n_latency_steps: 0
dataset_obs_steps: ${n_obs_steps}
keypoint_visible_rate: 1.0
obs_as_global_cond: True

policy:
  _target_: pcdp.policy.diffusion_PCDP_policy.PCDPPolicy
  num_action: ${horizon}
  input_dim: 6
  obs_feature_dim: 512
  hidden_dim: 512
  nheads: 8
  num_encoder_layers: 4
  num_decoder_layers: 1
  dropout: 0.1


ema:
  _target_: pcdp.model.diffusion.ema_model.EMAModel
  update_after_step: 0
  inv_gamma: 1.0
  power: 0.75
  min_value: 0.0
  max_value: 0.9999

dataloader:
  batch_size: 210
  num_workers: 15
  shuffle: True
  pin_memory: True
  persistent_workers: True
  drop_last: True


val_dataloader:
  batch_size: 128
  num_workers: 10
  shuffle: False
  pin_memory: True
  persistent_workers: False

optimizer:
  _target_: torch.optim.AdamW
  lr: 3.0e-4
  betas: [0.95, 0.999]
  eps: 1.0e-8
  weight_decay: 1.0e-6

training:
  device: "cuda:0"
  seed: 233
  debug: False
  resume: True
  num_epochs: 1500
  use_ema: True
  rollout_every: 5
  save_epochs: 20
  tqdm_interval_sec: 1.0
  validation_every: 5
  # new
  contrastive_lambda_max: 0.5
  contrastive_warmup_steps: 6000
  # normalization
  translation:
    - [-0.142, 0.585]    # X range (meters)
    - [-0.500, 0.330]   # Y range (meters)  
    - [-0.085, 0.470]    # Z range (meters)
  
logging:
  group: ${exp_name}
  id: null
  mode: online
  name: "moai_${now:%Y.%m.%d}-( nb_point: 15, STD: 1.7, Filter On)"
  project: pcdp
  resume: true
  tags:
  - stack3

checkpoint:
  save_ckpt: True
  save_last_ckpt: True # this only saves when save_ckpt is True
  save_last_snapshot: False

multi_run:
  run_dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
  wandb_name_base: ${now:%Y.%m.%D-%H.%M.%S}_${name}_${task_name}

hydra:
  job:
    override_dirname: ${name}
  run:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
  sweep:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
    subdir: ${hydra.job.num}
